import os
import sys
import time
import json
import random
import requests
import warnings
import unicodedata
import urllib.parse
import threading
from io import BytesIO
from PIL import Image
from dotenv import load_dotenv
import re # Importation de la biblioth√®que re pour les expressions r√©guli√®res

# Suppression des avertissements de d√©pr√©ciation (Gemini)
warnings.filterwarnings("ignore", category=FutureWarning, module="google.generativeai")

# --- Librairies Externes ---
import firebase_admin
from firebase_admin import credentials, firestore
import google.generativeai as genai
from playwright.sync_api import sync_playwright
from google.cloud.firestore_v1.base_query import FieldFilter

# Chargement des variables d'environnement (.env)
load_dotenv()

# --- CONFIGURATION ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
FACEBOOK_ACCESS_TOKEN = os.getenv("FACEBOOK_ACCESS_TOKEN")
FIREBASE_KEY_PATH = "serviceAccountKey.json"  # Doit √™tre √† la racine du projet
PROMPT_INSTRUCTION = "Evalue cette guitare Au quebec (avec le prix)."  # Instruction principale pour l'analyse IA
APP_ID_TARGET = os.getenv("APP_ID_TARGET")
USER_ID_TARGET = os.getenv("USER_ID_TARGET")

if not APP_ID_TARGET or not USER_ID_TARGET:
    print("‚ùå ERREUR: APP_ID_TARGET et USER_ID_TARGET doivent √™tre d√©finis dans le fichier .env")
    sys.exit(1)

# --- NOUVELLES INSTRUCTIONS SYST√àME ---
SYSTEM_PROMPT = """Tu es un luthier expert et un n√©gociant de guitares chevronn√© pour le march√© du Qu√©bec (MTL/QC).
Ton but : Analyser les photos pour prot√©ger l'acheteur contre les arnaques et les mauvais prix.

TA MISSION D'ANALYSE :
1.  **REJET (REJECTED)** : Si l'objet n'est pas une guitare/basse (ex: ampli, p√©dale, jouet, montre, guitare de jeu video). 
2.  **Authentification** : V√©rifie la forme de la t√™te (Headstock), le logo, le placement des boutons. Rep√®re les 'Chibson' ou contrefa√ßons.
3.  **√âtat** : Zoome sur les frettes (usure ?), le chevalet (oxydation ?), le manche (fissures ?).
4.  **Valeur** : Estime le prix de revente R√âALISTE au Qu√©bec (pas le prix neuf, le prix Kijiji/Marketplace).

FORMAT DE R√âPONSE ATTENDU (JSON) :
{
  "verdict": "GOOD_DEAL" | "FAIR" | "BAD_DEAL" | "REJECTED",
  "estimated_value": 1200,
  "confidence": 90,
  "reasoning": "Mod√®le 2018 authentique. Le prix demand√© (800$) est bien sous la cote habituelle (1100$). Attention : l√©g√®re scratch au dos.",
  "red_flags": ["Frettes tr√®s us√©es", "Bouton de volume non original"]
}
"""

# Initialisation Gemini
model = None
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(
        model_name='gemini-2.5-pro',
        system_instruction=SYSTEM_PROMPT,
        generation_config={
            "response_mime_type": "application/json", # Force le format JSON
            "temperature": 0.1 # Tr√®s bas pour une analyse froide et factuelle
        }
    )
else:
    print("‚ö†Ô∏è ATTENTION: Pas de cl√© API Gemini trouv√©e dans le fichier .env")

# Initialisation Firebase
db = None
offline_mode = False

if not firebase_admin._apps:
    try:
        if os.path.exists(FIREBASE_KEY_PATH):
            cred = credentials.Certificate(FIREBASE_KEY_PATH)
            print(f"üîë Projet ID d√©tect√© : {cred.project_id}")
            firebase_admin.initialize_app(cred)
            db = firestore.client()
            print("‚úÖ Firebase connect√© avec succ√®s (Database: Default).")
            
            # Test de permissions imm√©diat
            try:
                list(db.collections())
                print("‚úÖ Permissions de lecture confirm√©es sur la base.")
            except Exception as e:
                print(f"‚ùå ERREUR PERMISSIONS : {e}")
                print("üëâ Le compte de service n'a pas les droits. Passage en MODE HORS-LIGNE (Simulation).")
                offline_mode = True
        else:
            print(f"‚ö†Ô∏è Fichier {FIREBASE_KEY_PATH} introuvable. Passage en MODE HORS-LIGNE.")
            offline_mode = True

    except Exception as e:
        print(f"‚ùå Erreur critique Firebase: {e}")
        offline_mode = True


class GuitarHunterBot:
    def __init__(self, db_client, gemini_model, is_offline, prompt_instruction=PROMPT_INSTRUCTION):
        self.db = db_client
        self.model = gemini_model
        self.offline_mode = is_offline
        self.prompt_instruction = prompt_instruction
        
        # Configuration par d√©faut des r√®gles et du raisonnement
        self.verdict_rules = """- "GOOD_DEAL" : Le prix demand√© est INFERIEUR √† la valeur estim√©e.
- "FAIR" : Le prix demand√© est PROCHE de la valeur estim√©e (√† +/- 10%).
- "BAD_DEAL" : Le prix demand√© est SUPERIEUR √† la valeur estim√©e.
- "REJECTED" : L'objet n'est PAS ce que l'on recherche (ex: une montre guitare, un accessoire seul si on cherche une guitare, une guitare jouet, etc.)."""
        
        self.reasoning_instruction = "explication d√©taill√©e et compl√®te justifiant le verdict par rapport au prix et √† la valeur"

        # Configuration par d√©faut du scan
        self.scan_config = {
            "max_ads": 5,
            "frequency": 60, # minutes
            "location": "montreal",
            "distance": 60, # km
            "min_price": 0,
            "max_price": 10000,
            "search_query": "electric guitar"
        }
        self.last_refresh_timestamp = 0
        self.last_cleanup_timestamp = 0
        self.last_reanalyze_all_timestamp = 0 # Pour la nouvelle fonctionnalit√©
        self.city_mapping = {} # Sera rempli depuis Firestore

        # Construction du chemin pour v√©rification
        self.collection_path = f"artifacts/{APP_ID_TARGET}/users/{USER_ID_TARGET}/guitar_deals"
        
        print(f"\nüîß CONFIGURATION DU BOT :")
        print(f"   - APP ID  : {APP_ID_TARGET}")
        print(f"   - USER ID : {USER_ID_TARGET}")
        print(f"   - CHEMIN  : {self.collection_path}")
        print(f"   - PROMPT  : {self.prompt_instruction}")
        
        if self.offline_mode:
            print("‚ö†Ô∏è ATTENTION : MODE HORS-LIGNE ACTIV√â. Aucune donn√©e ne sera sauvegard√©e dans Firebase.")
            return

        # R√©f√©rence √† la collection sp√©cifique suivie par l'App React
        self.collection_ref = self.db.collection('artifacts').document(APP_ID_TARGET) \
            .collection('users').document(USER_ID_TARGET) \
            .collection('guitar_deals')
            
        # R√©f√©rence au document utilisateur pour √©couter les changements de prompt et config
        self.user_ref = self.db.collection('artifacts').document(APP_ID_TARGET) \
            .collection('users').document(USER_ID_TARGET)
            
        # R√©f√©rence √† la collection des villes
        self.cities_ref = self.db.collection('artifacts').document(APP_ID_TARGET) \
            .collection('users').document(USER_ID_TARGET) \
            .collection('cities')

        self._init_firestore_structure()

    def _init_firestore_structure(self):
        """Initialise la structure Firestore si elle n'existe pas."""
        print("   ‚è≥ V√©rification de l'acc√®s Firestore (Timeout 10s)...")
        try:
            # 1. Cr√©ation du document App (artifacts/{APP_ID})
            app_ref = self.db.collection('artifacts').document(APP_ID_TARGET)
            
            # V√©rification de la connexion avant de tenter des √©critures
            try:
                doc_snapshot = app_ref.get(timeout=10)
                
                if not doc_snapshot.exists:
                    app_ref.set({'created_at': firestore.SERVER_TIMESTAMP, 'type': 'app_root'})
                    print(f"üìÅ Document parent cr√©√© : artifacts/{APP_ID_TARGET}")
                else:
                    print("   ‚úÖ Connexion Firestore OK.")
                    
            except Exception as e:
                print(f"‚ùå Erreur de connexion Firebase lors de l'init : {e}")
                print("üëâ Passage en MODE HORS-LIGNE temporaire.")
                self.offline_mode = True
                return

            # 2. Cr√©ation du document User (artifacts/{APP_ID}/users/{USER_ID})
            user_ref = app_ref.collection('users').document(USER_ID_TARGET)
            if not user_ref.get(timeout=10).exists:
                user_ref.set({
                    'created_at': firestore.SERVER_TIMESTAMP, 
                    'type': 'user_root', 
                    'prompt': self.prompt_instruction,
                    'verdictRules': self.verdict_rules,
                    'reasoningInstruction': self.reasoning_instruction,
                    'scanConfig': self.scan_config
                })
                print(f"üë§ Document parent cr√©√© : users/{USER_ID_TARGET}")
            else:
                # Si le document existe, on r√©cup√®re le prompt et la config
                self.sync_configuration(initial=True)
                
            # Chargement initial des villes
            # self.load_cities_from_firestore() # Moved to main loop
                
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible de cr√©er les documents parents (non bloquant) : {e}")

    def load_cities_from_firestore(self):
        """Charge la liste des villes configur√©es par l'utilisateur depuis Firestore."""
        if self.offline_mode:
            return

        try:
            docs = self.cities_ref.stream()
            new_mapping = {}
            count = 0
            for doc in docs:
                data = doc.to_dict()
                if 'name' in data and 'id' in data:
                    # Normalisation du nom pour la recherche (minuscules, sans accents)
                    normalized_name = data['name'].lower().strip()
                    normalized_name = unicodedata.normalize('NFD', normalized_name).encode('ascii', 'ignore').decode("utf-8")
                    new_mapping[normalized_name] = data['id']
                    count += 1
            
            self.city_mapping = new_mapping
            print(f"   üèôÔ∏è {count} villes charg√©es depuis Firestore.")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur chargement des villes : {e}")

    def sync_configuration(self, initial=False):
        """Synchronise la configuration et v√©rifie les demandes de refresh."""
        if self.offline_mode:
            return False, False, False, None # Added None for specific_url_to_scan

        try:
            # Removed: self.load_cities_from_firestore()

            doc = self.user_ref.get()
            should_refresh = False
            should_cleanup = False
            should_reanalyze_all = False
            specific_url_to_scan = None # New variable

            if doc.exists:
                data = doc.to_dict()
                
                # --- MISE √Ä JOUR : Ajout des champs manquants ---
                update_payload = {}
                if 'verdictRules' not in data:
                    update_payload['verdictRules'] = self.verdict_rules
                if 'reasoningInstruction' not in data:
                    update_payload['reasoningInstruction'] = self.reasoning_instruction
                
                if update_payload:
                    print("üîß Mise √† jour du document utilisateur avec les nouveaux champs de configuration...")
                    self.user_ref.update(update_payload)
                    data.update(update_payload) # Met √† jour la copie locale des donn√©es
                # --- FIN MISE √Ä JOUR ---

                # 1. Prompt
                if 'prompt' in data and data['prompt'] != self.prompt_instruction:
                    self.prompt_instruction = data['prompt']
                    print(f"üîÑ Prompt mis √† jour : {self.prompt_instruction}")

                # 2. Verdict Rules
                if 'verdictRules' in data and data['verdictRules'] != self.verdict_rules:
                    self.verdict_rules = data['verdictRules']
                    print(f"üîÑ R√®gles de verdict mises √† jour.")

                # 3. Reasoning Instruction
                if 'reasoningInstruction' in data and data['reasoningInstruction'] != self.reasoning_instruction:
                    self.reasoning_instruction = data['reasoningInstruction']
                    print(f"üîÑ Instruction de raisonnement mise √† jour.")

                # 4. Scan Config
                if 'scanConfig' in data:
                    config = data['scanConfig']
                    self.scan_config['max_ads'] = config.get('maxAds', 5)
                    self.scan_config['frequency'] = config.get('frequency', 60)
                    self.scan_config['location'] = config.get('location', 'montreal')
                    self.scan_config['distance'] = config.get('distance', 60)
                    self.scan_config['min_price'] = config.get('minPrice', 0)
                    self.scan_config['max_price'] = config.get('maxPrice', 10000)
                    self.scan_config['search_query'] = config.get('searchQuery', 'electric guitar')

                # 5. Force Refresh
                if 'forceRefresh' in data:
                    last_refresh = data['forceRefresh']
                    
                    if initial:
                        # Initialisation : on se cale sur le timestamp actuel sans d√©clencher
                        self.last_refresh_timestamp = last_refresh
                    elif last_refresh != self.last_refresh_timestamp:
                        print(f"‚ö° Refresh manuel demand√© ! (Timestamp: {last_refresh})")
                        self.last_refresh_timestamp = last_refresh
                        should_refresh = True

                # 6. Force Cleanup
                if 'forceCleanup' in data:
                    last_cleanup = data['forceCleanup']
                    
                    if initial:
                        self.last_cleanup_timestamp = last_cleanup
                    elif last_cleanup != self.last_cleanup_timestamp:
                        print(f"üßπ Nettoyage manuel demand√© ! (Timestamp: {last_cleanup})")
                        self.last_cleanup_timestamp = last_cleanup
                        should_cleanup = True
                
                # 7. Force Reanalyze All
                if 'forceReanalyzeAll' in data:
                    last_reanalyze = data['forceReanalyzeAll']
                    if initial:
                        self.last_reanalyze_all_timestamp = last_reanalyze
                    elif last_reanalyze != self.last_reanalyze_all_timestamp:
                        print(f"üß† R√©-analyse globale demand√©e ! (Timestamp: {last_reanalyze})")
                        self.last_reanalyze_all_timestamp = last_reanalyze
                        should_reanalyze_all = True
                
                # 8. Scan Specific URL
                if 'scanSpecificUrl' in data and data['scanSpecificUrl']:
                    specific_url_to_scan = data['scanSpecificUrl']
                    print(f"üîó Scan d'URL sp√©cifique demand√© : {specific_url_to_scan}")

            return should_refresh, should_cleanup, should_reanalyze_all, specific_url_to_scan
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur sync config : {e}")
            return False, False, False, None

    def extract_facebook_id(self, url):
        """Extrait l'ID num√©rique unique de l'annonce Facebook depuis l'URL."""
        try:
            # Format typique: https://www.facebook.com/marketplace/item/1234567890/
            if "/item/" in url:
                # On coupe apr√®s /item/
                segment = url.split("/item/")[1]
                # On prend ce qu'il y a avant le prochain / ou ?
                fb_id = segment.split("/")[0].split("?")[0]
                if fb_id.isdigit():
                    return fb_id
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur extraction ID: {e}")
            return None

    def download_image(self, url):
        """T√©l√©charge l'image depuis l'URL et la convertit en objet PIL Image."""
        try:
            if not url or "via.placeholder.com" in url:
                return None
            
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                return Image.open(BytesIO(response.content))
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible de t√©l√©charger l'image : {e}")
            return None

    def analyze_deal_with_gemini(self, listing_data):
        """Utilise Gemini pour √©valuer si l'annonce est une bonne affaire (Multimodal)."""
        # Removed: self.sync_configuration()

        if not self.model:
             print("‚ö†Ô∏è Mod√®le Gemini non initialis√© (Cl√© API manquante ?)")
             return {
                "verdict": "ERROR",
                "estimated_value": listing_data['price'],
                "reasoning": "Analyse IA impossible : Mod√®le non initialis√©.",
                "confidence": 0
            }

        print(f"ü§ñ Analyse IA pour : {listing_data['title']}...")

        # T√©l√©chargement des images
        images = []
        # Gestion de plusieurs images (imageUrls) ou d'une seule (imageUrl)
        urls_to_process = listing_data.get('imageUrls', [])
        if not urls_to_process and listing_data.get('imageUrl'):
            urls_to_process = [listing_data['imageUrl']]
            
        # Limite √† 8 images pour √©viter de surcharger
        urls_to_process = urls_to_process[:8] # Changed from 5 to 8

        for url in urls_to_process:
            img = self.download_image(url)
            if img:
                images.append(img)
        
        # Le prompt devient une simple fiche technique
        user_message = f"""
        √âvalue cette annonce :
        Titre : {listing_data['title']}
        Prix affich√© : {listing_data['price']}$
        Description : {listing_data['description']}
        Localisation : {listing_data['location']}
        
        R√®gles de verdict √† appliquer :
        {self.verdict_rules}
        """

        try:
            # Construction du contenu multimodal
            content = [user_message]
            content.extend(images)
            
            if images:
                print(f"   üì∏ {len(images)} images incluses dans l'analyse.")
            else:
                print("   ‚ö†Ô∏è Analyse texte uniquement (pas d'image valide).")

            response = self.model.generate_content(content)
            # Pas besoin de nettoyer le markdown, response_mime_type est JSON
            result = json.loads(response.text)
            
            # --- CORRECTION : GESTION DU CAS O√ô GEMINI RENVOIE UNE LISTE ---
            if isinstance(result, list):
                if len(result) > 0:
                    return result[0]
                else:
                    return {
                        "verdict": "ERROR",
                        "estimated_value": listing_data['price'],
                        "reasoning": "L'IA a renvoy√© une liste vide.",
                        "confidence": 0
                    }
            return result

        except Exception as e:
            error_str = str(e)
            if "403" in error_str and "leaked" in error_str:
                print("\n" + "!"*60)
                print("‚ùå ERREUR CRITIQUE : VOTRE CL√â API GEMINI A FUIT√â ET EST BLOQU√âE.")
                print("üëâ Google a d√©sactiv√© cette cl√© par s√©curit√©.")
                print("üëâ G√©n√©rez-en une nouvelle ici : https://aistudio.google.com/app/apikey")
                print("üëâ Mettez √† jour GEMINI_API_KEY dans votre fichier .env")
                print("!"*60 + "\n")
            else:
                print(f"‚ùå Erreur Gemini: {e}")

            return {
                "verdict": "ERROR",
                "estimated_value": listing_data['price'],
                "reasoning": "Erreur d'analyse IA (Voir logs console)",
                "confidence": 0
            }

    def save_to_firestore(self, listing_data, analysis, doc_id=None):
        """Sauvegarde les donn√©es au chemin exact √©cout√© par React."""
        if self.offline_mode:
            print(f"üö´ [OFFLINE] Donn√©es non sauvegard√©es : {listing_data['title']}")
            return

        try:
            # Si pas d'ID fourni, on g√©n√®re un ID de secours (ne devrait pas arriver avec FB)
            if not doc_id:
                doc_id = f"{listing_data['title'][:15]}_{listing_data['price']}".replace(" ", "_").lower()
                doc_id = "".join(c for c in doc_id if c.isalnum() or c in ('_', '-'))

            # --- AJOUT LOGGING ---
            num_images_payload = len(listing_data.get('imageUrls', []))
            print(f"   üíæ Pr√©paration pour sauvegarde : {num_images_payload} images dans le payload.")

            # Si le verdict est REJECTED, on met le statut √† 'rejected'
            status = "analyzed"
            if analysis.get('verdict') == 'REJECTED':
                status = "rejected"

            data = {
                **listing_data,
                "aiAnalysis": analysis,
                "timestamp": firestore.SERVER_TIMESTAMP,
                "status": status
            }

            self.collection_ref.document(doc_id).set(data, merge=True) # Utiliser merge=True pour ne pas √©craser isFavorite
            print(f"üíæ Envoy√© √† l'App: {listing_data['title']} (ID: {doc_id}) - Status: {status}")
        except Exception as e:
            print(f"‚ùå Erreur Firestore: {e}")

    def cleanup_sold_listings(self):
        """V√©rifie si les annonces en base sont toujours disponibles et supprime les vendues."""
        if self.offline_mode:
            return

        print("\nüßπ Lancement du nettoyage des annonces vendues...")
        try:
            # 1. Get all non-rejected listings
            docs = self.collection_ref.where(filter=FieldFilter('status', '!=', 'rejected')).stream()
            
            listings_to_check = []
            for doc in docs:
                listings_to_check.append({'id': doc.id, 'data': doc.to_dict()})

            if not listings_to_check:
                print("   ‚úÖ Aucune annonce active √† v√©rifier.")
                return
                
            print(f"   üîç {len(listings_to_check)} annonces √† v√©rifier...")

            deleted_count = 0
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                
                # Utilisation d'un contexte similaire au scan pour √©viter les blocages
                context = browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
                    locale="fr-CA",
                    timezone_id="America/Montreal"
                )
                
                for listing in listings_to_check:
                    listing_id = listing['id']
                    listing_data = listing['data']
                    url = listing_data.get('link')

                    if not url:
                        continue

                    page = context.new_page()
                    try:
                        print(f"   - V√©rification de : {listing_data.get('title', listing_id)}...")
                        page.goto(url, timeout=30000)
                        
                        # Check for "This listing is no longer available"
                        # The text is "Cette annonce n‚Äôest plus disponible" in French
                        sold_indicator = page.locator('span:has-text("Cette annonce n‚Äôest plus disponible"), span:has-text("This listing is no longer available")')
                        
                        if sold_indicator.count() > 0:
                            print(f"   üóëÔ∏è Annonce vendue/supprim√©e. Suppression de la base de donn√©es...")
                            self.collection_ref.document(listing_id).delete()
                            deleted_count += 1
                        else:
                            # print(f"   üëç Annonce toujours active.")
                            pass

                    except Exception as e:
                        print(f"   ‚ö†Ô∏è Erreur lors de la v√©rification de {listing_id}: {e}")
                    finally:
                        page.close()
                        time.sleep(1) # Petite pause pour √™tre gentil avec le serveur

                browser.close()
            
            print(f"üèÅ Nettoyage termin√©. {deleted_count} annonce(s) supprim√©e(s).")

        except Exception as e:
            print(f"‚ùå Erreur durant le processus de nettoyage : {e}")

    def _close_login_popup(self, page):
        """Tente de fermer le popup de connexion qui peut appara√Ætre."""
        try:
            # On attend un peu que le popup se charge s'il doit appara√Ætre
            time.sleep(1) # R√©duit le d√©lai pour √™tre plus r√©actif
            close_login_btn = page.locator("div[aria-label='Fermer'], div[aria-label='Close'], div[role='button'][aria-label*='Fermer']").first
            
            if close_login_btn.count() > 0 and close_login_btn.is_visible(timeout=2000):
                print("   üîê Popup de connexion d√©tect√©, tentative de fermeture...")
                close_login_btn.click()
                print("   ‚úÖ Popup de connexion ferm√©.")
                time.sleep(1)
        except Exception as e:
            # Non bloquant, on continue
            pass

    def _setup_browser(self, p):
        """Initialise le navigateur et le contexte Playwright."""
        browser = p.chromium.launch(
            headless=False,
            args=["--start-minimized"] 
        )
        
        # Coordonn√©es de Montr√©al pour forcer la g√©olocalisation
        montreal_geo = {"latitude": 45.5017, "longitude": -73.5673}

        context = browser.new_context(
            viewport=None,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
            locale="fr-CA",
            timezone_id="America/Montreal",
            geolocation=montreal_geo,
            permissions=["geolocation"],
            extra_http_headers={"Referer": "https://www.google.com/"}
        )
        return browser, context

    def _apply_filters(self, page, distance, max_price):
        """Applique les filtres de rayon et de prix sur la page de recherche."""
        # --- APPLICATION DU RAYON VIA UI ---
        try:
            print(f"   üìç Tentative d'application du rayon de {distance} km via l'interface...")
            time.sleep(2)
            
            loc_btn = page.locator("div[role='button']").filter(has_text="km").first
            
            if loc_btn.count() > 0 and loc_btn.is_visible():
                loc_btn.click()
                time.sleep(2)
                
                modal = page.locator("div[role='dialog']").first
                if modal.count() > 0:
                    radius_dropdown = modal.locator("div, span").filter(has_text="kilom√®tres").last
                    if radius_dropdown.count() == 0:
                        radius_dropdown = modal.locator("div, span").filter(has_text="km").last
                    
                    if radius_dropdown.count() > 0:
                        radius_dropdown.click()
                        try: page.wait_for_selector("div[role='option']", timeout=5000)
                        except: pass

                        available_options = page.locator("div[role='option']").all()
                        best_option = None
                        min_diff = float('inf')
                        
                        visible_options = [opt for opt in available_options if opt.is_visible()]
                        
                        for option in visible_options:
                            text = option.inner_text()
                            digits = ''.join(filter(str.isdigit, text))
                            if digits:
                                val = int(digits)
                                diff = abs(val - distance)
                                if diff < min_diff:
                                    min_diff = diff
                                    best_option = option
                        
                        if best_option:
                            best_option.click()
                            time.sleep(1)
                    
                    apply_btn = modal.locator("div[aria-label*='Appliquer'], div[aria-label*='Apply'], span:has-text('Appliquer'), span:has-text('Apply')").first
                    if apply_btn.count() > 0:
                        apply_btn.click()
                        time.sleep(5)
                        try: page.wait_for_load_state("networkidle", timeout=5000)
                        except: pass
                    else:
                        page.keyboard.press("Escape")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur lors de l'application du rayon (UI) : {e}")

        # --- FOR√áAGE DE LA MISE √Ä JOUR VIA LE PRIX ---
        try:
            print(f"   üí∞ Application du prix final ({max_price} $) pour forcer la mise √† jour...")
            max_price_input = page.locator("input[aria-label='Prix maximum'], input[aria-label='Maximum price']").first
            
            if max_price_input.is_visible(timeout=5000):
                max_price_input.fill(str(max_price))
                time.sleep(0.5)
                max_price_input.press("Enter")
                print("   ‚úÖ Prix final appliqu√©. Attente du rechargement...")
                page.wait_for_load_state("networkidle", timeout=10000)
                time.sleep(3)
            else:
                print("   ‚ö†Ô∏è Champ 'Prix maximum' non trouv√© (timeout).")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur lors de l'application du prix final : {e}")

    def _extract_listing_details(self, context, clean_link, title, price, location):
        """Ouvre une annonce et extrait les d√©tails (images, description, et coordonn√©es)."""
        description = f"Annonce Marketplace. {title}. Localisation: {location}"
        image_urls = []
        coordinates = None # Initialisation des coordonn√©es √† None
        
        try:
            print(f"   ‚û°Ô∏è  Ouverture de l'annonce pour d√©tails : {clean_link}")
            detail_page = context.new_page()
            detail_page.goto(clean_link, timeout=45000)
            self._close_login_popup(detail_page)
            
            try: detail_page.wait_for_selector("div[role='main']", timeout=10000)
            except: pass
            time.sleep(2) 
            
            # --- RECUPERATION DES IMAGES ---
            collected_urls = []
            seen_srcs = set()
            
            for i in range(10): 
                try:
                    imgs = detail_page.locator("div[role='main'] img").all()
                    found_new = False
                    for img in imgs:
                        if not img.is_visible(): continue
                        box = img.bounding_box()
                        if box and box['width'] > 300 and box['height'] > 300:
                            src = img.get_attribute("src")
                            if src and "scontent" in src and src not in seen_srcs:
                                collected_urls.append(src)
                                seen_srcs.add(src)
                                found_new = True
                except: pass
                
                if not found_new and len(collected_urls) > 0: break
                if len(collected_urls) >= 10: break

                try:
                    detail_page.keyboard.press("ArrowRight")
                    time.sleep(0.8) 
                except: pass # Changed from `break` to `pass` to allow trying other image extraction methods
            
            image_urls = collected_urls
            
            # --- EXTRACTION DES COORDONN√âES DE LA CARTE ---
            try:
                map_image_locator = detail_page.locator('img[src*="staticmap"]').first
                if map_image_locator.is_visible(timeout=5000): # Attendre que l'image de la carte soit visible
                    src = map_image_locator.get_attribute('src')
                    if src:
                        # Utiliser une regex pour extraire les coordonn√©es dans l'URL
                        match = re.search(r'center=(-?\d+\.\d+)%2C(-?\d+\.\d+)', src)
                        if match:
                            coordinates = {"lat": float(match.group(1)), "lng": float(match.group(2))}
                            print(f"   üó∫Ô∏è Coordonn√©es extraites: {coordinates['lat']}, {coordinates['lng']}")
                        else:
                            print("   ‚ö†Ô∏è Coordonn√©es non trouv√©es dans l'URL de la carte statique.")
                else:
                    print("   ‚ö†Ô∏è Image de carte statique non trouv√©e sur la page de d√©tails.")
            except Exception as e:
                print(f"   ‚ùå Erreur lors de l'extraction des coordonn√©es de la carte: {e}")

            # --- RECUPERATION DESCRIPTION ---
            extracted_description = None
            try:
                og_desc = detail_page.locator('meta[property="og:description"]').get_attribute('content')
                if og_desc and len(og_desc.strip()) > 10:
                    extracted_description = og_desc.strip()
                else:
                    name_desc = detail_page.locator('meta[name="description"]').get_attribute('content')
                    if name_desc and len(name_desc.strip()) > 10:
                        extracted_description = name_desc.strip()

                if not extracted_description:
                    try:
                        see_more = detail_page.locator('div[role="button"]:has-text("Voir plus"), div[role="button"]:has-text("See more")').first
                        if see_more.is_visible(timeout=2000):
                            see_more.click()
                            time.sleep(0.5)

                        details_heading = detail_page.locator('h2:has-text("D√©tails"), h2:has-text("Details")').first
                        if details_heading.is_visible(timeout=1000):
                            parent = details_heading.locator('xpath=..')
                            all_texts = parent.locator('span[dir="auto"]').all_inner_texts()
                            long_texts = [t.strip() for t in all_texts if len(t.strip()) > 50]
                            if long_texts: extracted_description = max(long_texts, key=len)

                        if not extracted_description:
                            all_texts = detail_page.locator('div[role="main"] span[dir="auto"]').all_inner_texts()
                            excluded = {title, f"{price} $", location}
                            long_texts = [t.strip() for t in all_texts if len(t.strip()) > 50 and t not in excluded]
                            if long_texts: extracted_description = max(long_texts, key=len)

                    except: pass

            except: pass

            if extracted_description:
                description = extracted_description
            
            description = description[:3000]
            detail_page.close()
            
        except Exception as e:
            print(f"   ‚ùå Erreur d√©tails annonce : {e}")
            if 'detail_page' in locals():
                try: detail_page.close()
                except: pass
        
        return description, image_urls, coordinates # Modification de la valeur de retour

    def _process_listing(self, link_loc, context, location, distance, max_ads, processed_count, seen_urls):
        """Traite un √©l√©ment d'annonce individuel."""
        if processed_count >= max_ads:
            return processed_count

        href = link_loc.get_attribute("href")
        if not href: return processed_count
        full_link = f"https://www.facebook.com{href}" if href.startswith("/") else href
        clean_link = full_link.split('?')[0]
        
        if clean_link in seen_urls: return processed_count
        seen_urls.add(clean_link)

        fb_id = self.extract_facebook_id(clean_link)
        if not fb_id: return processed_count

        # Extraction basique
        text_content = link_loc.inner_text()
        lines = [line.strip() for line in text_content.split('\n') if line.strip()]
        
        img_loc = link_loc.locator("img").first
        image_url = "https://via.placeholder.com/400?text=No+Image"
        title = ""
        
        if img_loc.count() > 0:
            src = img_loc.get_attribute("src")
            if src: image_url = src
            alt_text = img_loc.get_attribute("alt")
            if alt_text and len(alt_text) > 3: title = alt_text

        if not title:
            for line in lines:
                if not any(c in line for c in ['$', '‚Ç¨', '¬£', 'Free', 'Gratuit']) and len(line) > 3:
                    title = line
                    break
            if not title: title = "Titre Inconnu"

        price = 0
        found_price = False
        for line in lines:
            if not found_price and any(c in line for c in ['$', '‚Ç¨', '¬£', 'Free', 'Gratuit']):
                digits = ''.join(filter(str.isdigit, line))
                if digits:
                    price = int(digits)
                    found_price = True
                elif "Free" in line or "Gratuit" in line:
                    price = 0
                    found_price = True
        
        # --- TENTATIVE D'EXTRACTION DE LA LOCALISATION SP√âCIFIQUE ---
        specific_location = location # Fallback
        if len(lines) >= 3:
            # Souvent la derni√®re ligne est la ville
            potential_loc = lines[-1]
            # Filtre basique pour √©viter de prendre un prix ou un statut
            if len(potential_loc) < 40 and not any(c in potential_loc for c in ['$', '‚Ç¨']):
                specific_location = potential_loc

        if (price > 0 or "Gratuit" in text_content or "Free" in text_content):
            print(f"   ‚ú® Annonce trouv√©e : {title} ({price} $) @ {specific_location}")
            
            # V√©rification doublon
            if not self.offline_mode:
                try:
                    doc_snap = self.collection_ref.document(fb_id).get()
                    if doc_snap.exists:
                        existing = doc_snap.to_dict()
                        if existing.get('status') == 'rejected':
                            print(f"   üö´ Annonce d√©j√† rejet√©e. On passe.")
                            return processed_count
                        if existing.get('price') == price:
                            print(f"   ‚è≠Ô∏è Annonce existante et prix inchang√©. On passe.")
                            return processed_count
                        print(f"   üîÑ Le prix a chang√© ! Mise √† jour...")
                except: pass
            
            # Extraction d√©taill√©e
            description, image_urls, coordinates = self._extract_listing_details(context, clean_link, title, price, location) # R√©cup√©ration des coordonn√©es
            final_image_url = image_urls[0] if image_urls else image_url

            listing_data = {
                "title": title,
                "price": price,
                "description": description,
                "imageUrl": final_image_url,
                "imageUrls": image_urls,
                "link": clean_link,
                "location": specific_location, # Utilisation de la localisation sp√©cifique
                "searchDistance": distance
            }
            if coordinates: # Ajout des coordonn√©es si elles ont √©t√© trouv√©es
                listing_data["latitude"] = coordinates["lat"]
                listing_data["longitude"] = coordinates["lng"]
            
            analysis = self.analyze_deal_with_gemini(listing_data)
            self.save_to_firestore(listing_data, analysis, doc_id=fb_id)
            return processed_count + 1
        
        return processed_count

    def scan_facebook_marketplace(self, search_query="electric guitar", location="montreal", distance=60, min_price=0, max_price=10000, max_ads=5):
        """Scrape r√©ellement Facebook Marketplace avec Playwright."""
        print(f"\nüåç Lancement du scan Facebook pour '{search_query}' √† {location} (Max: {max_ads}, Prix: {min_price}-{max_price}$)...")
        
        # --- VALIDATION DE LA VILLE ---
        normalized_loc = location.lower().strip()
        normalized_loc = unicodedata.normalize('NFD', normalized_loc).encode('ascii', 'ignore').decode("utf-8")
        city_id = self.city_mapping.get(normalized_loc)
        
        if not city_id:
            if location.isdigit(): city_id = location
            else:
                error_msg = f"Ville '{location}' inconnue. Ajoutez-la dans l'onglet Configuration."
                print(f"‚ùå {error_msg}")
                if not self.offline_mode:
                    try: self.user_ref.update({'scanStatus': 'error', 'scanError': error_msg})
                    except: pass
                return
        
        if not self.offline_mode:
            try: self.user_ref.update({'scanStatus': 'running', 'scanError': firestore.DELETE_FIELD})
            except: pass

        print(f"   üìç Ville identifi√©e : ID {city_id}")

        with sync_playwright() as p:
            browser, context = self._setup_browser(p)
            page = context.new_page()
            
            encoded_query = urllib.parse.quote(search_query)
            temp_max_price = max_price - 1 if max_price > min_price and max_price > 0 else max_price
            url = f"https://www.facebook.com/marketplace/{city_id}/search/?minPrice={min_price}&maxPrice={temp_max_price}&query={encoded_query}&exact=false"
            
            try:
                print(f"   ‚û°Ô∏è Navigation vers : {url}")
                page.goto(url, timeout=60000)
                
                try: page.evaluate("document.body.style.zoom = '0.5'")
                except: pass
                
                # Cookies
                try: page.get_by_role("button", name="Allow all cookies").click(timeout=3000)
                except: pass
                try: page.get_by_role("button", name="Decline optional cookies").click(timeout=3000)
                except: pass

                self._close_login_popup(page)
                self._apply_filters(page, distance, max_price)

                # Scroll
                try: page.wait_for_selector("div[role='main']", timeout=15000)
                except: pass
                print("   üìú D√©filement...")
                for _ in range(3):
                    page.mouse.wheel(0, 1000)
                    time.sleep(2)

                # Extraction
                print("   üîç Recherche des √©l√©ments d'annonce...")
                listings_locators = page.locator("a[href*='/marketplace/item/']").all()
                print(f"   üëÄ {len(listings_locators)} √©l√©ments trouv√©s.")
                
                processed_count = 0
                seen_urls = set()

                for link_loc in listings_locators:
                    processed_count = self._process_listing(link_loc, context, location, distance, max_ads, processed_count, seen_urls)
                    if processed_count >= max_ads: break
                        
            except Exception as e:
                print(f"‚ùå Erreur durant le scraping : {e}")
                import traceback
                traceback.print_exc()
            finally:
                browser.close()
                print("üèÅ Session de scraping termin√©e.")

    def scan_specific_url(self, url_to_scan):
        """Scanne une URL Facebook Marketplace sp√©cifique, extrait les d√©tails, analyse et sauvegarde."""
        if self.offline_mode:
            print(f"üö´ [OFFLINE] Impossible de scanner l'URL sp√©cifique : {url_to_scan}")
            return

        print(f"\nüîó Lancement du scan d'URL sp√©cifique : {url_to_scan}")

        # Clear the specific URL request in Firestore immediately
        try:
            self.user_ref.update({'scanSpecificUrl': firestore.DELETE_FIELD})
            print("   ‚úÖ Requ√™te d'URL sp√©cifique effac√©e de Firestore.")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur lors de l'effacement de scanSpecificUrl dans Firestore : {e}")

        fb_id = self.extract_facebook_id(url_to_scan)
        if not fb_id:
            print(f"‚ùå Impossible d'extraire l'ID Facebook de l'URL : {url_to_scan}")
            return

        with sync_playwright() as p:
            browser, context = self._setup_browser(p)
            try:
                detail_page = context.new_page()
                detail_page.goto(url_to_scan, timeout=60000)
                self._close_login_popup(detail_page)
                try: detail_page.wait_for_selector("div[role='main']", timeout=15000)
                except: pass
                time.sleep(2)

                # Extract title, price, location from the detail page
                title = "Titre Inconnu"
                price = 0
                location = "Localisation Inconnue"

                # Try to get title from meta tag first
                og_title = detail_page.locator('meta[property="og:title"]').get_attribute('content')
                if og_title:
                    title = og_title.split(' - ')[0] # Often "Title - Price - Location"

                # Try to get price
                price_locator = detail_page.locator('div[role="main"] span:has-text("$"]').first
                if price_locator.count() > 0:
                    price_text = price_locator.inner_text()
                    digits = ''.join(filter(str.isdigit, price_text))
                    if digits:
                        price = int(digits)

                # Try to get location
                location_locator = detail_page.locator('div[role="main"] span:has-text("¬∑")').first # Often "City ¬∑ Time"
                if location_locator.count() > 0:
                    location_text = location_locator.inner_text()
                    location = location_text.split('¬∑')[0].strip()


                description, image_urls, coordinates = self._extract_listing_details(context, url_to_scan, title, price, location)
                final_image_url = image_urls[0] if image_urls else "https://via.placeholder.com/400?text=No+Image"

                listing_data = {
                    "title": title,
                    "price": price,
                    "description": description,
                    "imageUrl": final_image_url,
                    "imageUrls": image_urls,
                    "link": url_to_scan,
                    "location": location,
                    "searchDistance": 0 # Not applicable for specific URL scan
                }
                if coordinates:
                    listing_data["latitude"] = coordinates["lat"]
                    listing_data["longitude"] = coordinates["lng"]

                analysis = self.analyze_deal_with_gemini(listing_data)
                self.save_to_firestore(listing_data, analysis, doc_id=fb_id)
                print(f"‚úÖ Scan d'URL sp√©cifique termin√© pour : {title}")

            except Exception as e:
                print(f"‚ùå Erreur lors du scan d'URL sp√©cifique {url_to_scan}: {e}")
                import traceback
                traceback.print_exc()
            finally:
                browser.close()

    def process_retry_queue(self):
        """Traite les annonces marqu√©es pour r√©-analyse."""
        if self.offline_mode: return

        try:
            # On cherche les annonces avec status='retry_analysis'
            docs = self.collection_ref.where(filter=FieldFilter('status', '==', 'retry_analysis')).stream()
            count = 0
            
            for doc in docs:
                data = doc.to_dict()
                print(f"üîÑ R√©-analyse demand√©e pour : {data.get('title', 'Sans titre')}")
                
                # On reconstruit listing_data √† partir de Firestore
                # On s'assure d'avoir les champs minimaux
                if not data.get('title'):
                    print("   ‚ö†Ô∏è Donn√©es incompl√®tes, impossible de r√©-analyser.")
                    continue
                    
                listing_data = {
                    "title": data.get('title'),
                    "price": data.get('price'),
                    "description": data.get('description', ''),
                    "location": data.get('location', 'Inconnue'),
                    "imageUrls": data.get('imageUrls', []),
                    "imageUrl": data.get('imageUrl'),
                    "link": data.get('link')
                }
                # Si les coordonn√©es existent d√©j√†, on les garde
                if data.get('latitude') is not None and data.get('longitude') is not None:
                    listing_data['latitude'] = data['latitude']
                    listing_data['longitude'] = data['longitude']
                
                # Analyse IA
                analysis = self.analyze_deal_with_gemini(listing_data)
                
                # Sauvegarde (mettra √† jour le status)
                self.save_to_firestore(listing_data, analysis, doc_id=doc.id)
                count += 1
                
            if count > 0:
                print(f"‚úÖ {count} annonces r√©-analys√©es.")
                
        except Exception as e:
            print(f"‚ùå Erreur lors du traitement de la file d'attente : {e}")

    def reanalyze_all_listings(self):
        """Marque toutes les annonces non rejet√©es pour une r√©-analyse."""
        if self.offline_mode:
            print("üö´ [OFFLINE] Impossible de lancer la r√©-analyse globale.")
            return

        print("üß† Lancement du processus de r√©-analyse globale...")
        try:
            docs = self.collection_ref.where(filter=FieldFilter('status', '!=', 'rejected')).stream()
            
            batch = self.db.batch()
            count = 0
            for doc in docs:
                batch.update(doc.reference, {'status': 'retry_analysis'})
                count += 1
            
            if count > 0:
                batch.commit()
                print(f"‚úÖ {count} annonces marqu√©es pour r√©-analyse. Le thread de surveillance va les traiter.")
            else:
                print("‚úÖ Aucune annonce active √† r√©-analyser.")

        except Exception as e:
            print(f"‚ùå Erreur lors du marquage pour r√©-analyse globale : {e}")

    def run_test_scan(self):
        """G√©n√®re des donn√©es de test pour v√©rifier la synchronisation."""
        print(f"üîé D√©marrage du scan de test (MOCK)...")

        mock_listings = [
            {
                "title": "Gibson Les Paul Standard 2021",
                "price": 1600,
                "description": "√âtat neuf, micros Burstbucker, √©tui original. Urgent.",
                "imageUrl": "https://images.unsplash.com/photo-1516924962500-2b4b3b99ea02?q=80&w=400",
                "imageUrls": [
                    "https://images.unsplash.com/photo-1516924962500-2b4b3b99ea02?q=80&w=400",
                    "https://images.unsplash.com/photo-1564186763535-ebb21ef5277f?q=80&w=400"
                ],
                "link": "https://facebook.com/marketplace/item/1234567890"
            },
            {
                "title": "Squier Strat Classic Vibe 60s",
                "price": 250,
                "description": "Excellent √©tat, parfaite pour d√©buter ou upgrade.",
                "imageUrl": "https://images.unsplash.com/photo-1550291652-6ea9114a47b1?q=80&w=400",
                "imageUrls": [
                    "https://images.unsplash.com/photo-1550291652-6ea9114a47b1?q=80&w=400"
                ],
                "link": "https://facebook.com/marketplace/item/0987654321"
            }
        ]

        for listing in mock_listings:
            # Extraction ID fictif
            fb_id = self.extract_facebook_id(listing['link'])
            analysis = self.analyze_deal_with_gemini(listing)
            self.save_to_firestore(listing, analysis, doc_id=fb_id)
            time.sleep(1)

def monitor_retries(bot):
    """Fonction ex√©cut√©e dans un thread s√©par√© pour surveiller les demandes de r√©-analyse."""
    print("üßµ D√©marrage du thread de surveillance des r√©-analyses...")
    while True:
        try:
            # On v√©rifie la file d'attente
            bot.process_retry_queue()
            # Pause courte pour ne pas spammer Firestore (5 secondes)
            time.sleep(5)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur dans le thread de surveillance : {e}")
            time.sleep(10) # Pause plus longue en cas d'erreur

if __name__ == "__main__":
    # Demande du prompt personnalis√© au d√©marrage
    print(f"Prompt par d√©faut: {PROMPT_INSTRUCTION}")
    
    bot = GuitarHunterBot(db, model, offline_mode)

    # --- S√âCURIT√â AU D√âMARRAGE ---
    if bot.offline_mode:
        print("\n‚ùå Le bot n'a pas pu s'initialiser correctement (mode hors-ligne). Arr√™t du script.")
        sys.exit(1)
    
    # --- D√âMARRAGE DU THREAD DE SURVEILLANCE ---
    retry_thread = threading.Thread(target=monitor_retries, args=(bot,), daemon=True)
    retry_thread.start()
    
    print("\n--- MODE AUTOMATIQUE ---")
    print("Le bot va surveiller la configuration et scanner p√©riodiquement.")
    print("Appuyez sur Ctrl+C pour arr√™ter.")
    
    last_scan_time = 0
    last_auto_cleanup_time = 0
    
    try:
        while True:
            # 1. Synchronisation de la config et v√©rification du refresh manuel
            should_refresh, should_cleanup, should_reanalyze_all, specific_url_to_scan = bot.sync_configuration()
            
            # NOTE: bot.process_retry_queue() est maintenant g√©r√© par le thread monitor_retries

            # --- GESTION DU SCAN D'URL SP√âCIFIQUE ---
            if specific_url_to_scan:
                bot.scan_specific_url(specific_url_to_scan)

            # 2. V√©rification du temps √©coul√©
            current_time = time.time()
            frequency_seconds = bot.scan_config['frequency'] * 60
            
            # --- GESTION DU NETTOYAGE (MANUEL OU QUOTIDIEN) ---
            # Nettoyage automatique une fois par jour (24h = 86400s)
            if should_cleanup or (current_time - last_auto_cleanup_time > 86400):
                if should_cleanup:
                    print("üßπ Lancement du nettoyage (Manuel)...")
                else:
                    print("üßπ Lancement du nettoyage quotidien (Auto)...")
                
                bot.cleanup_sold_listings()
                last_auto_cleanup_time = time.time()

            # --- GESTION DE LA R√â-ANALYSE GLOBALE ---
            if should_reanalyze_all:
                bot.reanalyze_all_listings()

            # --- GESTION DU SCAN P√âRIODIQUE ---
            # Si refresh demand√© OU temps √©coul√©
            if should_refresh or (current_time - last_scan_time > frequency_seconds):
                if should_refresh:
                    print("‚ö° Lancement du scan (Manuel)...")
                else:
                    print(f"‚è∞ Lancement du scan (Auto - {bot.scan_config['frequency']} min)...")
                
                # Load cities before every scan
                bot.load_cities_from_firestore() # Moved here

                # Lancement du scan
                bot.scan_facebook_marketplace(
                    search_query=bot.scan_config['search_query'],
                    location=bot.scan_config['location'],
                    distance=bot.scan_config['distance'],
                    min_price=bot.scan_config['min_price'],
                    max_price=bot.scan_config['max_price'],
                    max_ads=bot.scan_config['max_ads']
                )
                
                last_scan_time = time.time()
                print(f"üí§ Prochain scan auto dans {bot.scan_config['frequency']} minutes...")
            
            # Pause courte pour √©viter de spammer Firestore
            time.sleep(5)
            
    except KeyboardInterrupt:
        print("\nüõë Arr√™t du bot.")